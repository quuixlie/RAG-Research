from rag.llms.llm_factory import LLMFactory
import time



def answer_relevancy(question: str, answer: str, correct_answer: str, llm: LLMFactory, async_generate: bool = False) -> float:
    """
    Evaluate the answer relevancy using the LLM.

    Args:
        question (str): The question asked.
        answer (str): The answer generated by the RAG architecture.
        correct_answer (str): The correct answer to the question.
        llm (LLMFactory): The LLM to use for evaluation.
        async_generate (bool): Whether to use async evaluation.

    Returns:
        float: The relevancy score of the answer.
    """
    
    # Prompt template
    prompt = f"""You are an expert evaluator tasked with verifying the correctness of answers.

        Given the following:

        - Question: {question}
        - Generated Answer: {answer}
        - Correct Answer: {correct_answer}

        Determine whether the Generated Answer is factually correct and adequately answers the Question, with respect to the Correct Answer. Minor differences in phrasing, synonyms, or structure are acceptable as long as the meaning is the same. Include your reasoning in your response.
        """

    response = None
    if async_generate:
        response = llm.a_generate(prompt)
    else:
        response = llm.generate(prompt)

    # Check if generated answer is correct based on the correct answer and reasoning
    prompt = f"""
        You are a factual evaluator.
        Given the following:
        - Question: {question}
        - Reasoning: {response}

        Respond with exactly "yes" if the reasoning justifies that the answer is correct. Respond with exactly "no" if the reasoning shows that the answer is incorrect or insufficient. Do not output anything else."""

    if async_generate:
        response = llm.a_generate(prompt)
    else:
        response = llm.generate(prompt)

    # Parse the response
    try:
        relevancy_score = 1.0 if response.lower() == "yes" else 0.0
    except Exception as e:
        print(f"Error parsing response: {e}")
        relevancy_score = 0.0

    return relevancy_score