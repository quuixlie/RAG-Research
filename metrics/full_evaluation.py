import os
from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualPrecisionMetric, ContextualRecallMetric, HallucinationMetric
from rag.llms.llm_factory import LLMFactory


def full_evaluate(
    question: str,
    correct_answer: str,
    relevant_contexts: list[str],
    rag_answer: str,
    rag_contexts: list[str],
    evaluation_llm_name: str,
    llm_kwargs: dict,
) -> tuple[float, float, float, float, float]:
    """
    Evaluate the RAG architecture on the file.

    Args:
        question (str): The question asked.
        correct_answer (str): The correct answer to the question.
        relevant_contexts (list[str]): The relevant contexts for the question.
        rag_answer (str): The answer generated by the RAG architecture.
        rag_contexts (list[str]): The contexts used by the RAG architecture to generate the answer.

    Returns:
        tuple[float, float, float, float, float]: A tuple containing the accuracy, faithfulness, context recall, and context precision, hallucination.
    """
    llm_name = evaluation_llm_name
                     
    # Define the metrics
    metrics = [
        AnswerRelevancyMetric(
        ),
        FaithfulnessMetric(
        ),
        ContextualRecallMetric(
        ),
        ContextualPrecisionMetric(
        ),
        HallucinationMetric(
        ),
    ]

    # Create a test case
    test_case = LLMTestCase(
        input=question,
        actual_output=rag_answer,
        expected_output=correct_answer,
        retrieval_context=rag_contexts,
        context=relevant_contexts,
    )

    # Evaluate the test case
    result = evaluate(test_cases=[test_case], metrics=metrics)
    result = result.test_results[0]
    metrics_data = result.metrics_data

    # Extract metrics from the test result
    accuracy = next((m.score for m in metrics_data if m.name == "Answer Relevancy"), 0.0)
    faithfulness = next((m.score for m in metrics_data if m.name == "Faithfulness"), 0.0)
    context_recall = next((m.score for m in metrics_data if m.name == "Contextual Recall"), 0.0)
    context_precision = next((m.score for m in metrics_data if m.name == "Contextual Precision"), 0.0)
    hallucination = next((m.score for m in metrics_data if m.name == "Hallucination"), 0.0) 

    return accuracy, faithfulness, context_recall, context_precision, hallucination