from src.llms.llm import LLM


def faithfulness(question: str, answer: str, rag_contexts: list[str], llm: LLM) -> float:
    """
    Evaluate the faithfulness of the RAG architecture.

    Args:
        question (str): The question asked.
        answer (str): The answer generated by the RAG architecture.
        rag_contexts (list[str]): The contexts used by the RAG architecture to generate the answer.
        llm (LLMFactory): The LLM to use for evaluation.
    Returns:
        float: The faithfulness score between 0.0 and 1.0.
    """

    prompt = f"""
    You are a factual evaluator.
    Given the following:
    - Question: {question}
    - Answer: {answer}
    - Contexts: {rag_contexts}
    Determine whether the answer is faithful to the question and the contexts. Include your reasoning in your response.
    """
    response = llm.generate(prompt)

    # Check if the answer is faithful to the question and the contexts
    prompt = f"""
    You are a factual evaluator.
    Given the following:
    - Question: {question}
    - Answer: {answer}
    - Reasoning: {response}
    Respond with numbers between 0 and 1, where 0 means "not faithful at all" and 1 means "very faithful" based on the reasoning. Do not output anything else.
    """
    response = llm.generate(prompt)
    try:
        faithfulness_score = float(response)
    except ValueError:
        print("Error parsing response. Setting faithfulness score to 0.0.")
        faithfulness_score = 0.0
        # Imput to continue
        input("Do you want to continue? (y/n): ")

    return max(0.0, min(1.0, faithfulness_score))  # Ensure the score is between 0 and 1
